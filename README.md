# ğŸš€ Apache Spark - Data Lake com MinIO

Projeto de estudo sobre **Apache Spark para Engenharia de Dados** implementando um **Data Lake** completo usando **MinIO** como storage de objetos e **JupyterLab** como ambiente de desenvolvimento.

## ğŸ“‹ Sobre o Projeto

Este projeto implementa a arquitetura de **Data Lake** seguindo o padrÃ£o **Medallion Architecture** (Bronze, Silver, Gold), simulando um ambiente real de engenharia de dados com diferentes fontes de dados e formatos.

### ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Landing Zone  â”‚â”€â”€â”€â–¶â”‚   Bronze Zone   â”‚â”€â”€â”€â–¶â”‚   Silver Zone   â”‚â”€â”€â”€â–¶â”‚    Gold Zone    â”‚
â”‚   (Dados Brutos)â”‚    â”‚  (Dados Limpos) â”‚    â”‚(Dados Process.) â”‚    â”‚ (Dados Finais) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Tecnologias Utilizadas

- **ğŸ³ Docker & Docker Compose** - ContainerizaÃ§Ã£o do ambiente
- **ğŸ“¦ MinIO** - Storage de objetos compatÃ­vel com S3
- **ğŸ“Š JupyterLab** - Ambiente de desenvolvimento e anÃ¡lise
- **ğŸ Python** - Linguagem principal
- **ğŸ“ˆ Apache Spark** - Processamento distribuÃ­do (PySpark)
- **ğŸ­ Faker** - GeraÃ§Ã£o de dados sintÃ©ticos
- **ğŸ“‹ Pandas** - ManipulaÃ§Ã£o de dados
- **ğŸ¹ PyArrow** - Formato Parquet otimizado

## ğŸš€ Como Executar

### PrÃ©-requisitos

- Docker Desktop instalado e rodando
- Git instalado
- UV (gerenciador de pacotes Python) - opcional

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/eduardowanderleyde/spark-module1.git
cd spark-module1
```

### 2. Inicie o ambiente

```bash
docker compose up
```

### 3. Acesse as interfaces

- **JupyterLab**: http://localhost:8888
  - Token: `password`
- **MinIO Console**: http://localhost:9001
  - UsuÃ¡rio: `minioadmin`
  - Senha: `minioadmin`

### 4. Execute os scripts de configuraÃ§Ã£o

```bash
# Instalar dependÃªncias (se usando UV)
uv sync

# Criar buckets no MinIO
uv run src/02_setup/01_create_buckets.py

# Gerar dados de exemplo
uv run src/02_setup/02_landing_csv.py    # Dados CSV (Protheus)
uv run src/02_setup/03_landing_json.py   # Dados JSON (SAP)
uv run src/02_setup/04_landing_parquet.py # Dados Parquet (Cloud X)

# Gerar dados para outras zonas
uv run src/02_setup/03_bronze_zone.py    # Bronze Zone
uv run src/02_setup/04_silver_zone.py    # Silver Zone
uv run src/02_setup/05_gold_zone.py      # Gold Zone
```

## ğŸ“ Estrutura do Projeto

```
spark-module1/
â”œâ”€â”€ docker-compose.yml          # ConfiguraÃ§Ã£o do ambiente
â”œâ”€â”€ pyproject.toml              # DependÃªncias Python
â”œâ”€â”€ uv.lock                     # Lock file das dependÃªncias
â”œâ”€â”€ README.md                   # Este arquivo
â””â”€â”€ src/
    â””â”€â”€ 02_setup/
        â”œâ”€â”€ 01_create_buckets.py     # CriaÃ§Ã£o dos buckets
        â”œâ”€â”€ 02_landing_csv.py        # Dados CSV - Landing Zone
        â”œâ”€â”€ 03_landing_json.py       # Dados JSON - Landing Zone
        â”œâ”€â”€ 04_landing_parquet.py    # Dados Parquet - Landing Zone
        â”œâ”€â”€ 03_bronze_zone.py        # Dados limpos - Bronze Zone
        â”œâ”€â”€ 04_silver_zone.py        # Dados processados - Silver Zone
        â””â”€â”€ 05_gold_zone.py          # Dados finais - Gold Zone
```

## ğŸ—„ï¸ Estrutura dos Buckets

### Landing Zone (Dados Brutos)
- **`dataway/protheus/clients/`** - Dados CSV do sistema Protheus
- **`dataway/sap/clients/`** - Dados JSON do sistema SAP
- **`dataway/cloud_x/clients/`** - Dados Parquet do sistema Cloud X

### Bronze Zone (Dados Limpos)
- **`processed/protheus/`** - Dados estruturados e limpos

### Silver Zone (Dados Processados)
- **`enriched/sap/`** - Dados enriquecidos com objetos aninhados

### Gold Zone (Dados Finais)
- **`analytics/cloud_x/`** - Dados otimizados para analytics

## ğŸ“Š Tipos de Dados Gerados

### CSV (Protheus)
- Dados estruturados de clientes
- Campos: ID, nome, email, telefone, endereÃ§o, salÃ¡rio, etc.

### JSON (SAP)
- Dados aninhados com objetos complexos
- Estruturas: dados_pessoais, endereco, dados_profissionais, preferencias

### Parquet (Cloud X)
- Dados otimizados com mÃ©tricas calculadas
- Campos: score_credito, valor_vida_cliente, propensao_compra, etc.

## ğŸ”§ ConfiguraÃ§Ãµes

### MinIO
- **Porta API**: 9000
- **Porta Console**: 9001
- **Credenciais**: minioadmin / minioadmin

### JupyterLab
- **Porta**: 8888
- **Token**: password
- **Volume**: `./notebooks:/home/jovyan/work`

## ğŸ“š PrÃ³ximos Passos

1. **AnÃ¡lise com PySpark** - Processar dados usando Apache Spark
2. **ETL Pipelines** - Implementar pipelines de transformaÃ§Ã£o
3. **VisualizaÃ§Ãµes** - Criar dashboards com os dados
4. **ML Pipeline** - Implementar modelos de machine learning

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¨â€ğŸ’» Autor

**Eduardo Wanderley**
- GitHub: [@eduardowanderleyde](https://github.com/eduardowanderleyde)

---

â­ **Se este projeto foi Ãºtil para vocÃª, deixe uma estrela!**
